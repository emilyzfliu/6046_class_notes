\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=3cm,bottom=3cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{6.046 Review Notes}
\author{Emily Liu}

\begin{document}
\maketitle

\section{Robotic Coin Collection}
Problem statement: Given a MxN grid, there are squares with coins. Robots are allowed to travel from the top left (1,1) corner of the grid to the bottom right (M,N) corner of the grid, only by travelling to the right and downwards. We want to find the minimum number of robots required in order to collect all of the coins on the grid.
\\
\\
Our first intuition is to use a greedy strategy where the robot collects as many coins as possible on each run, which is easily achievable through dynamic programming. However, this is not ideal. One can picture this with a setup where the greedy strategy is for the robot to collect as many coins as possible running down middle but leaving coins at the sides which require multiple passes to collect, whereas the true optimal strategy is to use fewer suboptimal passes that do not intersect each other.
\\
\\
To approach this problem, we need to observe that if one coin is to the lower left of another coin, they must be collected on separate passes, since the robot can't go up or left to collect the second coin when the first one of the two is collected. It then follows that to ensure all coins are collected, we need to implement a `peeling' algorithm wherein the lower leftmost coins are collected first and the layer after that is collected next, etc (by symmetry, we can also collect the upper rightmost coins first).

\subsection*{Algorithm}
\begin{enumerate}
    \item Select the first column that contains a coin and go down to the bottom coin on that column.
    \item Go right to the nearest column that has any coins below the row the robot is currently on.
    \item Repeat until the bottom right corner is reached, and then repeat until all coins are collected. The number of robots used is the minimum possible.
\end{enumerate}

\subsection*{Analysis}
In order for the algorithm to work, the $r$ paths $p_1$ ... $p_r$ must cover all possible coins (1). For all paths, there must also exist a coin from each path such that all such coins form a disjoint set (2). This is because the minimum number of robots should be equal to the number of disjoint sets in the coins.
\\
\\
The first condition is easy to prove from the algorithm itself as the algorithm will not stop until all coins are collected. For the second condition, we consider two paths $p_i$, $p_j$ where $i < j$. For all coins in $p_j$, there exists a coin $c'$ in $p_i$ such that $(c, c')$ are disjoint, meaning that the x coordinate of $c'$ is less than that of $c$ and the y coordinate is greater. Thus, for any coin $c_r$ in the upper rightmost path $p_r$, we can find a coin in $p_{r-1}$ that is disjoint from $c_r$. By induction, we can find such coins in $p_{r-2}$ all the way to $p_1$. Therefore $r$ is the smallest number of robots possible. 
\newpage
\section{Median Finding (Divide and Conquer)}
Problem statement: Given an unsorted list of numbers, find the median. More generally, find the number of rank $i$ aka the $i$th smallest number.
\\
\\
We use a divide and conquer algorithm to solve this problem. Very generally, we want to select a pivot and group all numbers smaller than the pivot to the left and all numbers larger to the right. Then, we can very easily determine the rank of the pivot. If the rank of the pivot is too large, we recurse on the lesser (L) group with the same rank. If the rank is too small, we recurse on the greater (G) group with rank $i -$ rank$(x)$ where $x$ is the pivot point.
\\
\\
The trick is that we want to select a pivot intelligently so we can guarantee a geometric decrease in subproblem size (as opposed to a linear decrease) - this will converge faster.

\subsection*{Algorithm}
\begin{enumerate}
    \item First, we pad the end of the array with infinity values to make it a multiple of 10 (if necessary).
    \item Divide the array into groups of 5 and find the median of each group in constant time.
    \item Apply the first two steps to the set of group medians recursively to find the median of all of the numbers and select it as a pivot point. We observe that the number of elements either greater or less than the median will not exceed 7/10 of the overall number of elements - in other words, the median is 7/10 balanced. This also satisfies the condition we posed above.
    \item We partition all other elements into L and G as described above and recurse.
    \item The base case is when there are fewer than 10 elements in which case we can brute force in constant time.
\end{enumerate}

\subsection*{Analysis}
The runtime of the algorithm is given by $T(n) = O(n) + T(\frac{7n}{10}) + T(\frac{n}{5})$. We observe that the sum of coefficients of the recursion is less than 1, which hints at it being a linear algorithm. We prove by induction that it is linear. The inductive hypothesis is $T(n) = O(n) \leq c_1 n$.

\begin{align*}
    T(n) &= O(n) + T(\frac{7n}{10}) + T(\frac{n}{5})\\
    &= c_2 n + \frac{7}{10} c_1 n + \frac{n}{5} c_1 n \\
    &= c_2 n + \frac{9}{10} c_1 n \\
    &= c_1 n + (c_2 - \frac{1}{10} n \leq c_1 n
\end{align*}
\\
if we appropriately select $c_1 \geq 10 c_2$.

\newpage
\section{Fast Multiplication (Karatsuba's Method)}
This algorithm applies a divide and conquer technique to perform multiplication of two $n$ digit numbers in faster than $O(n^2)$ time.
\\
\\
To gain intuition, consider the two digit product $ab \times cd$. This is equal to $ac \times 100 + cd + (bc + ad) \times 10$. However, we note that $bc + ad = (a + b) (c + d) - ac - bd$.
\\
\\
We extend this to higher digit numbers. We can recursively apply the algorithm to find $ac$, $bd$, and $(a+b)(c+d)$ (we can obtain the sum in $O(n)$ time). We then obtain $bc + ad$ in $O(n)$ time by the subtraction operation.
\subsection*{Analysis}
The algorithm runtime is defined by $T(n) = 3 T(n/2) + \theta (n)$, which evaluates to $\theta(n^{\log_2 3})$ by the Master Theorem.
\section{Matrix Multiplication Checking}
We implement a random algorithm to check if $AB = C$ modulo, where $A, B,$ and $C$ are all $n \times n$ matrices. The algorithm is guaranteed to return true if $AB = C$ and returns true over $1/2$ of the time if $AB \neq C$. We can run the algorithm $k << n$ times to improve the confidence of our answer.
\subsection*{Algorithm (Frievald)}
The algorithm is to generate a random $n \times 1$ vector $r$ and perform the computations $A(Br)$ and $Cr$ in $O(n^2)$ time. If $ABr = Cr$, return True; otherwise, return False.
\subsection*{Analysis}
It is trivial to see why $ABr = Cr$ always if $AB = C$. In the case where $ABr \neq Cr$, we consider the matrix $D = AB - C$ and want to find the probability that $Dr \neq 0$ given $D \neq 0$.\\
\\
We observe that for any possible $D \neq 0$, we can select a one-hot $v$ such that $Dv \neq 0$ just by matching the position equal to 1 with a nonzero element in $D$. Then, for every $r_{bad}$ such that $D r_{bad} = 0$, we can identify a new vector $r' = r_{bad} + v$ such that $D r' \neq 0$.\\
\\
Now, we need to show that each $r_{bad}$ has a corresponding $r'$ that is unique. Namely, if $r' = r + v = q + v$ and $Dr = Dq = 0$, then we prove that $r = q$. For every index $j = [1...n]$, we get $r_j' = (r_j + v_j) mod 2 = (q_j + v_j) mod 2$, and therefore $r_j = q_j$ for all $j$.
\newpage
\section{Union Find}
Suppose we have a collection of sets. We want a data structure that can perform the operations MAKE\_SET($x$), which creates a new set with a new element, UNION($x, y$), which merges the set containing $x$ and the set containing $y$, and FIND\_SET($x$), which returns the set containing $x$. We can select one element in each set to serve as the ID for that set.
\subsection*{Algorithm}
MAKE\_SET($x$) is trivially $O(1)$ since we create a new set with only the element $x$ (we assume $x$ is guaranteed to be a new element; otherwise we can use the other functions to check if $x$ is already in the set).\\
\\
We can use a shallow tree data structure to store the sets. Each set ID element is the root of the tree. In FIND\_SET, we assume we have a pointer to the element in the data structure. We find the path to the root in $O(h)$ time. To maintain shallowness of the tree, we point each element we traverse directly to the root of the tree.\\
\\
In the UNION operation, we first perform FIND\_SET to find the two sets, then point the head of the smaller tree to the head of the larger tree, thereby merging the two sets. This is also $O(h)$ which we attempt to minimize through the parent-reassigning operation in FIND\_SET.
\\
\section{Self-Organizing lists (Competitive Analysis)}
We wish to maintain a list $L$ consisting of $N$ elements with distinct keys. We want this list to support one operation, ACCESS($x$), by finding the element $x$ in the list. After accessing the element, we reorder the list by transposing adjacent elements.\\
\\
The strategy we use is the move-to-front (MTF) strategy in which we take the most recently accessed element and move it to the front, and adjust all other elements as necessary by shifting them down.
\subsection*{Analysis}
To analyze, we compare the cost of the online algorithm to that of the optimal offline algorithm (competitive analysis). The online algorithm is said to be $\alpha$-competitive to the offline algorithm if $c_{online} \leq \alpha c_{offline} + k$ where $\alpha$ and $k$ are both constants independent of $|L|$.\\
\\
We will prove that the MTF strategy is 4-competitive. At each step, the cost of MTF of retrieving the $i$th element and adjusting the array is $c_i = 2 \text{rank}_{L_i}(x_i) - 1$ and the optimal cost is $c_i^* = 2 \text{rank}_{L_i^*}(x_i) + t_i$.
\\
We use the potential function $\Phi = 2$ [\# inversions between $L_i$ and $L_i^*$], where an inversion is defined by a pair $(x, y)$ such that $\text{rank}_{L_i}(x) < \text{rank}_{L_i^*}(y)$ and $\text{rank}_{L_i^*}(x) > \text{rank}_{L_i}(y)$. The number of transpositions needed to convert $L_i$ to $L_i^*$ is lower bounded by the number of inversions between ($L_i$, $L_i^*$).\\
\\
Now, we will define $A$ as the set of all elements that are before $x_i$ in both $L_i$ and $L_i^*$, $B$ as the set of elements after $x_i$ in $L_i^*$ but before in $L_i$, $C$ as the set of elements after $x_i$ in $L_i$ but before in $L_i^*$, and $D$ as the set of elements after $x_i$ in both $L_i$ and $L_i^*$. Then, rank$_{L_i}(x_i)$ = $|A| + |B| + 1$ and rank$_{L_i^*}(x_i)$ = $|A| + |C| + 1$.\\
\\
After moving $x_i$ to the front of the array in MTF, the elements in $B$ are no longer inversions but the elements in $A$ are new inversions, meaning $\Delta \Phi \leq 2(|A| - |B| + t_i)$. We then evaluate
\begin{align*}
    \hat{c_i} &= c_i + \Delta \Phi\\
    &= 2r - 1 + 2(|A| - |B| + t_i) &r = |A| + |B| + 1\\
    \hat{c_i} &\leq 4|A| + 1 + 2 t_i &r^* = |A| + |C| + 1 \geq |A| + 1 \implies |A| = r^* - 1\\
    \hat{c_i} &\leq 4 (r^* - 1)  + 1 + 2 t_i \leq 4 (r^* + t_i) = 4 c_i^*
\end{align*}
\section{Hashing I}
We are already familiar with hashing. In 6.006, we operated under the simplified uniform hashing assumption which assumes that the distribution of keys to hash function values will always be uniformly random. We determined that the average access time is $O(1 + \alpha)$ where $\alpha = \frac{n}{m}$, $n$ is the number of entries in the table, and $m$ is the table capacity. The issue with a naive formulation (such as modulo) is that the function can be easily exploited such that all entries are hash collisions.
\subsection*{Universal Hashing}
Suppose we have a hash family $\mathcal{H}$ consisting of many hypotheses $h$. If a hash family is universal, then it means that
\begin{align*}
    P_{h \in \mathcal{H}} (h(k_1) = h(k_2)) \leq \frac{1}{m}
\end{align*}
for all pairs of fixed keys $k_1 \neq k_2$.
\subsection*{Theorem: Universal hashing and $O(1 + \alpha)$}
For $n$ arbitrary distinct keys $k_1 ... k_n$ and a random $h \in \mathcal{H}$,
\begin{align*}
    \mathbb{E}_h\left[\text{\# keys that collide w/ any } k_i\right] \leq 1 + \alpha = 1 + \frac{n}{m}.
\end{align*}
We will prove this by linearity of expectation. Let $I_{ij} = \mathbbm{1}\left[h(k_i) = h(k_j)\right]$.
\begin{align*}
    \mathbb{E}_h\left[\text{\# keys that collide w/ } k_i\right] &= \mathbb{E}_{h \in \mathcal{H}} \sum_j I_{ij}\\
    &= \sum_j \mathbb{E}_{h \in \mathcal{H}}I_{ij} = \sum_j P(h(k_i) = h(k_j)\\
    &= 1 + \sum_{j \neq i} P(h(k_i) = h(k_j) = 1 + \frac{n-1}{m} = 1 + \alpha.
\end{align*}
Therefore, a universal hash family is guaranteed to uphold the optimal expected hash runtime.
\subsection*{Constructing a Universal Hash Family: Dot Product Hash Function}
How do we construct a universal hash function? One way is to populate the hash table with random uniformly generated values from $0$ to $m-1$. The issue with this is that a universal hash table would occupy a space of $\Theta(|U|)$ (where $|U|$ is the size of the universe, or the set of all hashable things), meaning it is highly impractical.\\
\\
The dot product hash family is a universal hash family that takes up less space and is more practical to implement. To implement the dot product hash function, we will first assume $m$ is prime and $|U| = m^r$. We can easily pad $U$ to be of size $m^r$ if it is not already. Then, we let the set of keys $\mathbf{k} = (k_0, k_1, ... k_{r-1})$ and the descriptions (coefficients) be $\mathbf{a} = (a_0, a_1, ... a_{r-1})$. Note that both $\mathbf{k}$ and $\mathbf{a}$ are vectors, so we define our hash function as $h_{\mathbf{a}} (\mathbf{k}) = \mathbf{a} \cdot \mathbf{k} \mod m = \sum_{i=0}^{r-1} a_i k_i \mod m$.\\
\\
The hash family $\mathcal{H}$ is described as the set of all $h_{\mathbf{a}}$ where all values of $\mathbf{a}$ are between 0 and $m-1$. Now, the space it occupies is $\Theta(r) = \Theta(\log |U|)$, which is more practical than $\Theta(|U|)$.\\
\\
Now, we need to prove that the dot product family is universal, or $P_h(h(k) = h(k')) \leq \frac{1}{m}$, where $k, k' \in \mathbb{R}^r$. Without loss of generality, assume that $k$ and $k'$ differ at their first location ($k_0 \neq k'_0$). We then have
\begin{align*}
    P_\mathbf{a}(h_\mathbf{a}(k) = h_\mathbf{a} (k')) &= P_\mathbf{a} (\mathbf{a} \cdot k = \mathbf{a} \cdot k' \mod m)\\
    &= P_\mathbf{a} (a_0 (k_0 - k'_0) = \sum_{i=1}^{r-1} a_i (k_i' - k_i) \mod m)
\end{align*}
Here, we note that the first term is guaranteed to not be 0 since $k_0 \neq k'_0$. That means we can calculate the modulo inverse $(k_0 - k'_0)^{-1}$ which is unique since $m$ is prime. This means that our expression evaluates to $P_\mathbf{a} (a_0 = (k_0 - k'_0)^{-1} \sum_{i=1}^{r-1} a_i (k_i' - k_i) \mod m)$, meaning that the expression on the right is independent of $a_0$. This means that we can first pick $a_1 ... a_{r-1}$ at random and then pick $a_0$ at random independently (principle of deferred decisions). If we let $(k_0 - k'_0)^{-1} \sum_{i=1}^{r-1} a_i (k_i' - k_i) \mod m = k$, then we can see that $P(a_0 = k) = \frac{1}{m}$ and therefore the dot product hash family is universal.
\section{Hashing II}
We will continue to discuss different hashing strategies.
\subsection*{Open Addressing}
Open addressing is when instead of having a hashing function $h(x)$, you have a probing function $p(x, i)$ that instructs you how to reselect an entry in the hash table if the current entry is occupied.\\
When you select a key, you first compute $p(x, 0)$. If the table at $p(x, 0)$ is vacant, you can place the key there. If occupied, you keep incrementing $i$ until you reach a $p(x, i)$ that is vacant.\\
\\
When deleting elements from the table, you need to place a marker indicating an element was deleted so the search operation for the hash set knows to continue looking further in the sequence. However, a spot belonging to a deleted element can still be overwritten by new inserts.
\subsubsection*{Uniform hashing assumption}
Assume that every probing function generates a sequence $p(k, 0)$, ... $p(k, m-1)$ that is a uniformly random permutation of $\{0 ... m-1\}$. Then, the probability that each $p(k, i)$ hits an empty slot is $1 - \alpha$, meaning the expected number of probes you will need to perform is upper-bounded by $\frac{1}{1 - \alpha}$.
\subsubsection*{Open Addressing in Practice}
One example of open addressing is linear probing, where $p(k, i) = h(k) + i \mod m$ and $h$ is a hash function selected from a uniform hash family. The issue with linear probing is that it results in clusters of occupied spaces that could compromise efficiency. Quadratic probing where $p(k, i) = h(k) + i^2 \mod m$ breaks up these clusters. Double hashing is where you select two hash functions $h_1$ and $h_2$ from the uniform hash family and $p(k, i) = h_1(k) + i\times h_2(k) \mod m$.
\subsection*{Perfect Hashing}
Perfect Hashing is a solution to a static dictionary problem where you are given $n$ keys to build a hash table in $O(n) \log ^c (n)$ build time that occupies $O(n)$ space and has $O(1)$ worst case search time. Since it is a static dictionary, no keys will be deleted.\\
\\
We will accomplish this using a hash table of capacity $m$ and associated hash function $h_1$. For each spot $j$, we point to a hash table of size $l_j^2$ where $l_j$ is the number of collisions that occur at position $j$ and associated hash $h_{2,j}$. If $\sum l_j^2 < cn$ for some predetermined $c$, we reselect $h_1$. Then, we select hash functions for the smaller hash tables, and if there is a collision in the small hash table at any $j$, we reselect $h_{2, j}$. Therefore, we only need 2 accesses to search for any element in the table.
\subsubsection*{Analysis}
We look at the expected number of times we will need to reselect $h_2$. For a fixed $j$, the probability $P(h_{2, j}(k) = h_{2, j}(k')) \leq {l_j \choose 2} \times \frac{1}{l_j^2} \leq \frac{1}{2}$, meaning that the expected number of times you would need to reselect is 2 for each $j$.\\
\\
We now look at the expected number of times we need to reselect $h_1$, or the probability that $\mathbb{E}(\sum l_j^2) \leq c n$. Observing that $l_j$ elements means $l_j \choose 2$ collisions under $h_1$, $\mathbb{E}(\sum l_j^2) \leq \mathbb{E}($number of collisions under $h_1) + O(n)$. This is less than or equal to ${n \choose 2}*\frac{1}{m} + O(n) = c' n$, meaning that the probability of reselecting is bounded by $\frac{c'}{c}$ via Markov.
\end{document}